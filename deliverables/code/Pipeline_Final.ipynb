{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Pipeline CATS 12\n",
    "\n",
    "### Lucas Jollie, Michelle Klein, Maaike Scholten, Folkert Stijnman\n",
    "\n",
    "1. Extract features based on Chi Squared test and literature\n",
    "2. Validate Naive Bayes, Random Forest, MLP\n",
    "3. Visualise in boxplot\n",
    "4. Train model and output predictions/model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          0    1    2    3    4    5    6    7    8    9     ... 2824 2825  \\\n",
      "Array.129    0    0    0    0    0    0    0    0    0    0  ...    2    2   \n",
      "Array.34     0    0    0    0    0    0    0    0    0    0  ...    1    1   \n",
      "Array.67     0    0    0    0    0    0    0    0    0    0  ...    1    1   \n",
      "Array.24     0    0    0    0    0    0    0   -1    0    0  ...    0    0   \n",
      "Array.22     0    0    0    0    0    0    0    0    0    0  ...    1    1   \n",
      "...        ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   \n",
      "Array.10     0    0    0    0    0    0    0    0    0    0  ...    1    1   \n",
      "Array.123    0    0    0    0    0    0    0    0    0    0  ...    1    1   \n",
      "Array.100    0    0    0    0    0    0    0    0    0    0  ...    1    1   \n",
      "Array.134   -1   -1   -1   -1   -1   -1   -1   -1   -1   -1  ...    1    1   \n",
      "Array.130    0    0    0    0    0    0    0    0    0    0  ...    1    1   \n",
      "\n",
      "          2826 2827 2828 2829 2830 2831 2832 2833  \n",
      "Array.129    2    2    0    1    1    1    1    1  \n",
      "Array.34     1    1    1    1    1    1    1    1  \n",
      "Array.67     1    1    1    1    1    1    1    1  \n",
      "Array.24     0    0    0    0    0    0    0    0  \n",
      "Array.22     1    1    1    1    1    1    1    1  \n",
      "...        ...  ...  ...  ...  ...  ...  ...  ...  \n",
      "Array.10     0    1    1    1    1    1    1    1  \n",
      "Array.123    1    1    1    1    1    1    1    1  \n",
      "Array.100    1    1    1    1    1    1    1    1  \n",
      "Array.134    1    1    1    1    1    1    1    1  \n",
      "Array.130    1    1    1    1    1    1    1    1  \n",
      "\n",
      "[100 rows x 2834 columns]\n",
      "[1, 2, 2, 3, 3, 2, 1, 1, 3, 1, 2, 3, 1, 1, 3, 3, 2, 1, 2, 2, 3, 1, 2, 1, 2, 1, 1, 3, 1, 2, 2, 1, 2, 1, 2, 2, 3, 3, 3, 1, 2, 3, 2, 3, 2, 3, 1, 1, 3, 3, 1, 2, 2, 3, 2, 3, 1, 1, 3, 2, 1, 2, 3, 1, 2, 1, 2, 1, 1, 2, 1, 1, 2, 2, 2, 3, 1, 3, 2, 3, 3, 3, 3, 2, 1, 1, 2, 3, 3, 3, 3, 2, 3, 2, 3, 1, 2, 2, 2, 1]\n",
      "The regions found in the literature were:  [111, 157, 249, 65, 361, 360, 479, 576, 583, 688, 664, 625, 670, 772, 876, 877, 878, 937, 991, 992, 993, 966, 1136, 1137, 1092, 1207, 1234, 1370, 1371, 1387, 1296, 1310, 1583, 1407, 1575, 1513, 1589, 1611, 1697, 1645, 1657, 1725, 1734, 1735, 1865, 1904, 1911, 2017, 1965, 2015, 2016, 2207, 2074, 2075, 2306, 2184, 2135, 2136, 2200, 2201, 2071, 2160, 2161, 2446, 2681, 2682, 2683, 2684, 2685, 2732, 2744, 2794, 2822]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.feature_selection import f_classif\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV, cross_val_score, KFold, StratifiedKFold\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report, accuracy_score, make_scorer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "# 1 Load data and important genes\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "temp_list = []\n",
    "\n",
    "with open(\"train_call.txt\", 'r') as temp:\n",
    "    for line in temp:\n",
    "        temp_list.append(line.split())\n",
    "\n",
    "columns_temp = temp_list[0]\n",
    "columns_temp = [x.replace(\"\\\"\", \"\") for x in columns_temp]\n",
    "\n",
    "df_train = pd.DataFrame(temp_list[1:], columns=columns_temp)\n",
    "X = df_train.drop(['Chromosome', 'Start','End', 'Nclone'], axis=1)\n",
    "X = X.transpose()\n",
    "\n",
    "labels = []\n",
    "\n",
    "with open(\"Train_clinical.txt\", 'r') as temp_labels:\n",
    "    next(temp_labels)\n",
    "    for line in temp_labels:\n",
    "        temp = line.strip()\n",
    "        temp = temp.split()\n",
    "        if temp[1].strip(\"\\\"\") == \"HER2+\":\n",
    "            labels.append(1)\n",
    "        if temp[1].strip(\"\\\"\") == \"HR+\":\n",
    "            labels.append(2)\n",
    "        if temp[1].strip(\"\\\"\") == \"Triple\":\n",
    "            labels.append(3)\n",
    "\n",
    "Y = labels\n",
    "\n",
    "# Find the important genes list here if needed (anyone at VU can view this link): https://docs.google.com/spreadsheets/d/18T0gJ6nX67aa7veYpRTn_0yymVCvFnnllGqiQyqsIcc/edit?usp=sharing\n",
    "important_genes = [111, 157, 249, 65, 361, 360, 479, 576, 583, 688, 664, 625, 670, 772, 876, 877, 878, 937, 991, 992, 993, 966, 1136, 1137, 1092, 1207, 1234, 1370, 1371, 1387, 1296, 1310, 1583, 1407, 1575, 1513, 1589, 1611, 1697, 1645, 1657, 1725, 1734, 1735, 1865, 1904, 1911, 2017, 1965, 2015, 2016, 2207, 2074, 2075, 2306, 2184, 2135, 2136, 2200, 2201, 2071, 2160, 2161, 2446, 2681, 2682, 2683, 2684, 2685, 2732, 2744, 2794, 2822]\n",
    "            \n",
    "print(X)\n",
    "print(Y)\n",
    "print(\"The regions found in the literature were: \", important_genes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Gene  Region number       P-value Yes/No\n",
      "0              DIRAS3            111  6.897062e-01     No\n",
      "1              PTPN22            157  7.287985e-01     No\n",
      "2                 AGT            249  9.791761e-01     No\n",
      "3               CLSPN             65  5.039604e-01     No\n",
      "4   BARD1/CASP8/CTLA4            361  8.022867e-01     No\n",
      "5               SF3B1            360  8.717109e-01     No\n",
      "6              PIK3CA            479  4.007464e-01     No\n",
      "7               NFKB1            576  8.076397e-01     No\n",
      "8                FGF2            583  8.076397e-01     No\n",
      "9               RAD50            688  1.138701e-01     No\n",
      "10             MAP3K1            664  1.569463e-01     No\n",
      "11               TERT            625  9.757757e-01     No\n",
      "12             PIK3R1            670  4.250528e-02    Yes\n",
      "13              CCND3            772  1.568220e-01     No\n",
      "14               ESR1            876  4.297669e-01     No\n",
      "15               ESR1            877  3.435048e-01     No\n",
      "16               ESR1            878  3.384654e-01     No\n",
      "17               EGFR            937  9.492498e-01     No\n",
      "18       KMT2C (MLL3)            991  8.738841e-01     No\n",
      "19       KMT2C (MLL3)            992  8.280285e-01     No\n",
      "20       KMT2C (MLL3)            993  8.014328e-01     No\n",
      "21          CAV1/CAV2            966  8.425206e-01     No\n",
      "22                NBN           1136  7.630498e-01     No\n",
      "23                NBN           1137  7.911207e-01     No\n",
      "24              IKBKB           1092  7.408158e-02     No\n",
      "25              PTPRD           1207  5.060686e-01     No\n",
      "26               MELK           1234  6.914782e-01     No\n",
      "27               PTEN           1370  9.529703e-01     No\n",
      "28               PTEN           1371  7.162040e-01     No\n",
      "29              FGFR2           1387  9.025203e-01     No\n",
      "30              GATA3           1296  6.855018e-01     No\n",
      "31              MASTL           1310  4.611954e-01     No\n",
      "32                ATM           1583  2.250590e-01     No\n",
      "33           H19/LSP1           1407  5.048502e-01     No\n",
      "34           MRE11(A)           1575  2.436539e-01     No\n",
      "35              CCND1           1513  6.078381e-01     No\n",
      "36              TIRAP           1589  1.729887e-01     No\n",
      "37             CDKN1B           1611  7.665508e-01     No\n",
      "38               TBX3           1697  7.967035e-01     No\n",
      "39               KRT5           1645  1.479597e-01     No\n",
      "40               IL22           1657  5.859564e-02     No\n",
      "41              BRCA2           1725  6.931973e-01     No\n",
      "42                RB1           1734  3.485173e-01     No\n",
      "43                RB1           1735  4.047287e-01     No\n",
      "44               AKT1           1865  3.883914e-01     No\n",
      "45              RAD51           1904  2.001524e-01     No\n",
      "46            CYP19A1           1911  2.163115e-01     No\n",
      "47               CDH1           2017  4.623894e-02    Yes\n",
      "48              PALB2           1965  5.683650e-01     No\n",
      "49               CBFB           2015  1.517658e-01     No\n",
      "50               CDH3           2016  1.160794e-01     No\n",
      "51              BRCA1           2207  3.346706e-03    Yes\n",
      "52               TP53           2074  3.249625e-02    Yes\n",
      "53               TP53           2075  3.249625e-02    Yes\n",
      "54              BRIP1           2306  6.918517e-01     No\n",
      "55       ERBB2 (HER2)           2184  8.326227e-14    Yes\n",
      "56                NF1           2135  2.972004e-01     No\n",
      "57                NF1           2136  6.509545e-01     No\n",
      "58              KRT14           2200  4.172152e-01     No\n",
      "59              KRT17           2201  6.104122e-01     No\n",
      "60             ALOX15           2071  9.275122e-02     No\n",
      "61               CCL5           2160  5.475658e-01     No\n",
      "62               CCL4           2161  6.103237e-01     No\n",
      "63         STK11/LKB1           2446  9.669922e-01     No\n",
      "64              RUNX1           2681  5.234851e-01     No\n",
      "65              RUNX1           2682  4.492747e-01     No\n",
      "66              RUNX1           2683  5.987332e-01     No\n",
      "67              RUNX1           2684  6.175287e-01     No\n",
      "68              RUNX1           2685  5.718320e-01     No\n",
      "69              CHEK2           2732  4.082143e-02    Yes\n",
      "70              PDGFB           2744  9.509540e-02     No\n",
      "71                 AR           2794  7.771286e-01     No\n",
      "72               AFF2           2822  9.299035e-01     No\n",
      "Number of features = 204\n"
     ]
    }
   ],
   "source": [
    "## FEATURE SELECTION\n",
    "\n",
    "#Loading data\n",
    "patients = X.values.tolist()\n",
    "\n",
    "# we need to shift the features by 1 since the chi2 function does not take non-negative values\n",
    "patients_shift = []\n",
    "\n",
    "for patient in patients:\n",
    "    x = []\n",
    "    for feature in patient:\n",
    "        x.append(int(feature) + 1)\n",
    "    patients_shift.append(x)\n",
    "\n",
    "chi, p_val = chi2(patients_shift, Y)\n",
    "\n",
    "imp_genes_p_values = []\n",
    "\n",
    "for i, j in enumerate(important_genes):\n",
    "    if p_val[j] >= 0.05:\n",
    "        imp_genes_p_values.append([imp_genes[i][0], j, p_val[j], \"No\"])\n",
    "    else:\n",
    "        imp_genes_p_values.append([imp_genes[i][0], j, p_val[j], \"Yes\"])\n",
    "    \n",
    "\n",
    "important_genes_scores = pd.DataFrame(imp_genes_p_values, columns=[\"Gene\", \"Region number\", \"P-value\", \"Yes/No\"])\n",
    "pd.set_option('display.max_rows', 75)\n",
    "print(important_genes_scores)\n",
    "\n",
    "X_feature_selected = X\n",
    "\n",
    "threshold = 0.05\n",
    "for i, val in enumerate(p_val):\n",
    "    if val >= threshold and i not in important_genes:\n",
    "        X_feature_selected = X_feature_selected.drop([i], axis=1)\n",
    "        \n",
    "X = X_feature_selected\n",
    "number_of_features = len(X.values.tolist()[0])\n",
    "print(\"Number of features = {}\".format(number_of_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation scheme\n",
    "\n",
    "def nested_cross_validation(MODEL, PARAMS, X, Y, NUM_TRIALS):\n",
    "    nested_scores = []\n",
    "    \n",
    "    for i in range(NUM_TRIALS):\n",
    "        print(\"Running Trial {}...\".format(i + 1))\n",
    "        # K Fold inner & outer loop\n",
    "        inner_cv = StratifiedKFold(n_splits=4, shuffle=True, random_state=i)\n",
    "        outer_cv = StratifiedKFold(n_splits=4, shuffle=True, random_state=i)\n",
    "        \n",
    "        # one vs. rest approach\n",
    "        model_to_set = OneVsRestClassifier(MODEL)\n",
    "        \n",
    "        model = GridSearchCV(estimator=model_to_set, param_grid=PARAMS, cv=inner_cv, n_jobs=2)\n",
    "        nested_score = cross_val_score(model, X=X, y=Y, cv=outer_cv)\n",
    "        nested_scores.append([nested_score.mean(), nested_score.std()])\n",
    "        print(nested_score.mean(), nested_score.std())\n",
    "        \n",
    "    return nested_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Trial 1...\n",
      "0.8099999999999999 0.09165151389911684\n",
      "Running Trial 2...\n",
      "0.8099999999999999 0.13564659966250533\n",
      "Running Trial 3...\n",
      "0.8300000000000001 0.05099019513592784\n",
      "Running Trial 4...\n",
      "0.8099999999999999 0.08000000000000003\n",
      "Running Trial 5...\n",
      "0.8099999999999999 0.058309518948453015\n",
      "Running Trial 6...\n",
      "0.79 0.058309518948453015\n",
      "Running Trial 7...\n",
      "0.8099999999999999 0.07348469228349537\n",
      "Running Trial 8...\n",
      "0.8400000000000001 0.08602325267042628\n",
      "Running Trial 9...\n",
      "0.78 0.10295630140987\n",
      "Running Trial 10...\n",
      "0.8 0.05477225575051662\n",
      "Running Trial 11...\n",
      "0.8399999999999999 0.07999999999999999\n",
      "Running Trial 12...\n",
      "0.86 0.07999999999999999\n",
      "Running Trial 13...\n",
      "0.8 0.10488088481701516\n",
      "Running Trial 14...\n",
      "0.8099999999999999 0.12409673645990857\n",
      "Running Trial 15...\n",
      "0.8 0.0632455532033676\n",
      "Running Trial 16...\n",
      "0.8300000000000001 0.0678232998312527\n",
      "Running Trial 17...\n",
      "0.8400000000000001 0.12409673645990854\n",
      "Running Trial 18...\n",
      "0.8400000000000001 0.048989794855663564\n",
      "Running Trial 19...\n",
      "0.82 0.08124038404635962\n",
      "Running Trial 20...\n",
      "0.82 0.10295630140987003\n",
      "Running Trial 21...\n",
      "0.8 0.0632455532033676\n",
      "Running Trial 22...\n",
      "0.8300000000000001 0.06782329983125265\n",
      "Running Trial 23...\n",
      "0.82 0.0678232998312527\n",
      "Running Trial 24...\n",
      "0.76 0.10198039027185572\n",
      "Running Trial 25...\n",
      "0.76 0.066332495807108\n",
      "Running Trial 26...\n",
      "0.8 0.10954451150103321\n",
      "Running Trial 27...\n",
      "0.85 0.031622776601683784\n",
      "Running Trial 28...\n",
      "0.8600000000000001 0.08602325267042628\n",
      "Running Trial 29...\n",
      "0.8400000000000001 0.08602325267042626\n",
      "Running Trial 30...\n",
      "0.8300000000000001 0.0812403840463596\n",
      "Running Trial 31...\n",
      "0.8 0.13038404810405296\n",
      "Running Trial 32...\n",
      "0.8300000000000001 0.05099019513592784\n",
      "Running Trial 33...\n",
      "0.8300000000000001 0.05999999999999996\n",
      "Running Trial 34...\n",
      "0.85 0.04472135954999579\n",
      "Running Trial 35...\n",
      "0.8699999999999999 0.08717797887081347\n",
      "Running Trial 36...\n",
      "0.8099999999999999 0.08602325267042626\n",
      "Running Trial 37...\n",
      "0.8200000000000001 0.07483314773547885\n",
      "Running Trial 38...\n",
      "0.8300000000000001 0.06782329983125265\n",
      "Running Trial 39...\n",
      "0.7899999999999999 0.03741657386773941\n",
      "Running Trial 40...\n",
      "0.7899999999999999 0.08602325267042626\n",
      "Running Trial 41...\n",
      "0.8300000000000001 0.05099019513592784\n",
      "Running Trial 42...\n",
      "0.78 0.09273618495495704\n",
      "Running Trial 43...\n",
      "0.8 0.08366600265340758\n",
      "Running Trial 44...\n",
      "0.8 0.0\n",
      "Running Trial 45...\n",
      "0.8099999999999999 0.05830951894845301\n",
      "Running Trial 46...\n",
      "0.82 0.10295630140986999\n",
      "Running Trial 47...\n",
      "0.8099999999999999 0.11135528725660046\n",
      "Running Trial 48...\n",
      "0.79 0.05830951894845301\n",
      "Running Trial 49...\n",
      "0.8099999999999999 0.08602325267042626\n",
      "Running Trial 50...\n",
      "0.76 0.04898979485566356\n",
      "Running Trial 51...\n",
      "0.79 0.08602325267042626\n",
      "Running Trial 52...\n",
      "0.8300000000000001 0.0678232998312527\n",
      "Running Trial 53...\n",
      "0.8099999999999999 0.08602325267042626\n",
      "Running Trial 54...\n",
      "0.8399999999999999 0.12806248474865697\n",
      "Running Trial 55...\n",
      "0.8099999999999999 0.07348469228349536\n",
      "Running Trial 56...\n",
      "0.8300000000000001 0.06\n",
      "Running Trial 57...\n",
      "0.8299999999999998 0.0678232998312527\n",
      "Running Trial 58...\n",
      "0.79 0.058309518948453015\n",
      "Running Trial 59...\n",
      "0.8399999999999999 0.07999999999999999\n",
      "Running Trial 60...\n",
      "0.8 0.07745966692414831\n",
      "Running Trial 61...\n",
      "0.82 0.06782329983125267\n",
      "Running Trial 62...\n",
      "0.82 0.10770329614269007\n",
      "Running Trial 63...\n",
      "0.78 0.10295630140987003\n",
      "Running Trial 64...\n",
      "0.77 0.11224972160321824\n",
      "Running Trial 65...\n",
      "0.8300000000000001 0.08717797887081347\n",
      "Running Trial 66...\n",
      "0.8000000000000002 0.08944271909999162\n",
      "Running Trial 67...\n",
      "0.8099999999999999 0.10677078252031313\n",
      "Running Trial 68...\n",
      "0.8300000000000001 0.05099019513592784\n",
      "Running Trial 69...\n",
      "0.8400000000000001 0.05830951894845297\n",
      "Running Trial 70...\n",
      "0.79 0.13190905958272917\n",
      "Running Trial 71...\n",
      "0.8 0.07745966692414832\n",
      "Running Trial 72...\n",
      "0.8300000000000001 0.08124038404635958\n",
      "Running Trial 73...\n",
      "0.82 0.09797958971132713\n",
      "Running Trial 74...\n",
      "0.8300000000000001 0.03999999999999998\n",
      "Running Trial 75...\n",
      "0.7899999999999999 0.12000000000000001\n",
      "Running Trial 76...\n",
      "0.8600000000000001 0.03741657386773941\n",
      "Running Trial 77...\n",
      "0.82 0.0678232998312527\n",
      "Running Trial 78...\n",
      "0.8299999999999998 0.0812403840463596\n",
      "Running Trial 79...\n",
      "0.7700000000000001 0.13638181696985854\n",
      "Running Trial 80...\n",
      "0.79 0.09695359714832659\n",
      "Running Trial 81...\n",
      "0.8099999999999999 0.11575836902790225\n",
      "Running Trial 82...\n",
      "0.78 0.12884098726725127\n",
      "Running Trial 83...\n",
      "0.85 0.04472135954999579\n",
      "Running Trial 84...\n",
      "0.82 0.11224972160321826\n",
      "Running Trial 85...\n",
      "0.77 0.174928556845359\n",
      "Running Trial 86...\n",
      "0.8099999999999999 0.11575836902790225\n",
      "Running Trial 87...\n",
      "0.8100000000000002 0.12000000000000001\n",
      "Running Trial 88...\n",
      "0.8299999999999998 0.06782329983125265\n",
      "Running Trial 89...\n",
      "0.79 0.058309518948453015\n",
      "Running Trial 90...\n",
      "0.8099999999999999 0.09695359714832659\n",
      "Running Trial 91...\n",
      "0.8099999999999999 0.019999999999999973\n",
      "Running Trial 92...\n",
      "0.8299999999999998 0.11224972160321822\n",
      "Running Trial 93...\n",
      "0.8299999999999998 0.0678232998312527\n",
      "Running Trial 94...\n",
      "0.8399999999999999 0.06633249580710798\n",
      "Running Trial 95...\n",
      "0.8400000000000001 0.08602325267042626\n",
      "Running Trial 96...\n",
      "0.8299999999999998 0.09797958971132709\n",
      "Running Trial 97...\n",
      "0.8099999999999999 0.08602325267042626\n",
      "Running Trial 98...\n",
      "0.79 0.03741657386773941\n",
      "Running Trial 99...\n",
      "0.8400000000000001 0.07348469228349533\n",
      "Running Trial 100...\n",
      "0.8 0.08944271909999157\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes\n",
    "\n",
    "nb_nested = []\n",
    "\n",
    "for i in range(100):\n",
    "    print(\"Running Trial {}...\".format(i + 1))\n",
    "    cv_folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=i)\n",
    "    nb = OneVsRestClassifier(GaussianNB())\n",
    "    nested_score = cross_val_score(nb, X=X, y=Y, cv=cv_folds)\n",
    "    print(nested_score.mean(), nested_score.std())\n",
    "    nb_nested.append([nested_score.mean(), nested_score.std()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Trial 1...\n",
      "0.7600000000000001 0.04898979485566356\n",
      "Running Trial 2...\n",
      "0.71 0.18841443681416772\n",
      "Running Trial 3...\n",
      "0.77 0.07141428428542847\n",
      "Running Trial 4...\n",
      "0.7200000000000001 0.0748331477354788\n",
      "Running Trial 5...\n",
      "0.74 0.1183215956619923\n",
      "Running Trial 6...\n",
      "0.8 0.06324555320336758\n",
      "Running Trial 7...\n",
      "0.75 0.09110433579144296\n",
      "Running Trial 8...\n",
      "0.7900000000000001 0.08660254037844387\n",
      "Running Trial 9...\n",
      "0.78 0.060000000000000005\n",
      "Running Trial 10...\n",
      "0.73 0.07141428428542851\n",
      "Running Trial 11...\n",
      "0.78 0.06\n",
      "Running Trial 12...\n",
      "0.77 0.07681145747868608\n",
      "Running Trial 13...\n",
      "0.75 0.08660254037844387\n",
      "Running Trial 14...\n",
      "0.8 0.16248076809271922\n",
      "Running Trial 15...\n",
      "0.76 0.07483314773547882\n",
      "Running Trial 16...\n",
      "0.77 0.08660254037844387\n",
      "Running Trial 17...\n",
      "0.7500000000000001 0.091104335791443\n",
      "Running Trial 18...\n",
      "0.78 0.060000000000000005\n",
      "Running Trial 19...\n",
      "0.76 0.15748015748023622\n",
      "Running Trial 20...\n",
      "0.73 0.08660254037844387\n",
      "Running Trial 21...\n",
      "0.77 0.051961524227066326\n",
      "Running Trial 22...\n",
      "0.73 0.07681145747868606\n",
      "Running Trial 23...\n",
      "0.79 0.01732050807568879\n",
      "Running Trial 24...\n",
      "0.77 0.033166247903554026\n",
      "Running Trial 25...\n",
      "0.77 0.043588989435406726\n",
      "Running Trial 26...\n",
      "0.75 0.09949874371066199\n",
      "Running Trial 27...\n",
      "0.81 0.07141428428542851\n",
      "Running Trial 28...\n",
      "0.8 0.06324555320336758\n",
      "Running Trial 29...\n",
      "0.77 0.1244989959798873\n",
      "Running Trial 30...\n",
      "0.78 0.08717797887081347\n",
      "Running Trial 31...\n",
      "0.78 0.0447213595499958\n",
      "Running Trial 32...\n",
      "0.77 0.08660254037844387\n",
      "Running Trial 33...\n",
      "0.75 0.07681145747868608\n",
      "Running Trial 34...\n",
      "0.71 0.07141428428542852\n",
      "Running Trial 35...\n",
      "0.77 0.07681145747868608\n",
      "Running Trial 36...\n",
      "0.75 0.033166247903554026\n",
      "Running Trial 37...\n",
      "0.75 0.059160797830996134\n",
      "Running Trial 38...\n",
      "0.76 0.0848528137423857\n",
      "Running Trial 39...\n",
      "0.76 0.028284271247461926\n",
      "Running Trial 40...\n",
      "0.77 0.09110433579144296\n",
      "Running Trial 41...\n",
      "0.79 0.01732050807568879\n",
      "Running Trial 42...\n",
      "0.7300000000000001 0.051961524227066305\n",
      "Running Trial 43...\n",
      "0.75 0.07681145747868608\n",
      "Running Trial 44...\n",
      "0.7400000000000001 0.0447213595499958\n",
      "Running Trial 45...\n",
      "0.79 0.033166247903553984\n",
      "Running Trial 46...\n",
      "0.74 0.11489125293076054\n",
      "Running Trial 47...\n",
      "0.7300000000000001 0.051961524227066305\n",
      "Running Trial 48...\n",
      "0.68 0.03999999999999998\n",
      "Running Trial 49...\n",
      "0.77 0.033166247903554026\n",
      "Running Trial 50...\n",
      "0.7 0.04472135954999579\n",
      "Running Trial 51...\n",
      "0.79 0.033166247903553984\n",
      "Running Trial 52...\n",
      "0.79 0.08660254037844387\n",
      "Running Trial 53...\n",
      "0.76 0.07483314773547882\n",
      "Running Trial 54...\n",
      "0.76 0.06324555320336757\n",
      "Running Trial 55...\n",
      "0.7500000000000001 0.05916079783099614\n",
      "Running Trial 56...\n",
      "0.74 0.020000000000000018\n",
      "Running Trial 57...\n",
      "0.73 0.05916079783099617\n",
      "Running Trial 58...\n",
      "0.7600000000000001 0.04898979485566356\n",
      "Running Trial 59...\n",
      "0.7700000000000001 0.0714142842854285\n",
      "Running Trial 60...\n",
      "0.7400000000000001 0.034641016151377525\n",
      "Running Trial 61...\n",
      "0.8 0.09797958971132711\n",
      "Running Trial 62...\n",
      "0.7500000000000001 0.04358898943540673\n",
      "Running Trial 63...\n",
      "0.75 0.051961524227066326\n",
      "Running Trial 64...\n",
      "0.77 0.033166247903554026\n",
      "Running Trial 65...\n",
      "0.79 0.05916079783099617\n",
      "Running Trial 66...\n",
      "0.74 0.08717797887081347\n",
      "Running Trial 67...\n",
      "0.78 0.0447213595499958\n",
      "Running Trial 68...\n",
      "0.75 0.1244989959798873\n",
      "Running Trial 69...\n",
      "0.73 0.10723805294763607\n",
      "Running Trial 70...\n",
      "0.7600000000000001 0.09797958971132711\n",
      "Running Trial 71...\n",
      "0.75 0.059160797830996134\n",
      "Running Trial 72...\n",
      "0.8 0.0632455532033676\n",
      "Running Trial 73...\n",
      "0.74 0.06633249580710801\n",
      "Running Trial 74...\n",
      "0.81 0.043588989435406726\n",
      "Running Trial 75...\n",
      "0.77 0.11090536506409415\n",
      "Running Trial 76...\n",
      "0.7899999999999999 0.08660254037844385\n",
      "Running Trial 77...\n",
      "0.76 0.0692820323027551\n",
      "Running Trial 78...\n",
      "0.74 0.08246211251235319\n",
      "Running Trial 79...\n",
      "0.74 0.06633249580710796\n",
      "Running Trial 80...\n",
      "0.77 0.04358898943540673\n",
      "Running Trial 81...\n",
      "0.8300000000000001 0.06557438524302002\n",
      "Running Trial 82...\n",
      "0.74 0.07211102550927977\n",
      "Running Trial 83...\n",
      "0.7500000000000001 0.051961524227066326\n",
      "Running Trial 84...\n",
      "0.77 0.04358898943540673\n",
      "Running Trial 85...\n",
      "0.7400000000000001 0.08246211251235319\n",
      "Running Trial 86...\n",
      "0.73 0.08660254037844387\n",
      "Running Trial 87...\n",
      "0.81 0.05916079783099616\n",
      "Running Trial 88...\n",
      "0.76 0.028284271247461926\n",
      "Running Trial 89...\n",
      "0.76 0.028284271247461926\n",
      "Running Trial 90...\n",
      "0.8 0.08485281374238571\n",
      "Running Trial 91...\n",
      "0.79 0.05196152422706631\n",
      "Running Trial 92...\n",
      "0.8 0.03999999999999998\n",
      "Running Trial 93...\n",
      "0.78 0.09165151389911681\n",
      "Running Trial 94...\n",
      "0.72 0.1095445115010332\n",
      "Running Trial 95...\n",
      "0.78 0.07211102550927977\n",
      "Running Trial 96...\n",
      "0.79 0.05196152422706631\n",
      "Running Trial 97...\n",
      "0.78 0.07211102550927977\n",
      "Running Trial 98...\n",
      "0.7200000000000001 0.04898979485566356\n",
      "Running Trial 99...\n",
      "0.7500000000000001 0.05916079783099614\n",
      "Running Trial 100...\n",
      "0.73 0.07141428428542848\n"
     ]
    }
   ],
   "source": [
    "# Random Forest\n",
    "\n",
    "n_estimators = [int(x) for x in np.linspace(start = 1, stop = 50, num = 10, dtype=int)]\n",
    "max_features = ['auto', 'sqrt']\n",
    "\n",
    "parameters = {'estimator__n_estimators': n_estimators,\n",
    "               'estimator__max_features': max_features}\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "rf_nested = nested_cross_validation(rf, parameters, X, Y, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Trial 1...\n",
      "0.8500000000000001 0.08185352771872453\n",
      "Running Trial 2...\n",
      "0.82 0.04472135954999579\n",
      "Running Trial 3...\n",
      "0.81 0.05916079783099617\n",
      "Running Trial 4...\n",
      "0.84 0.028284271247461888\n",
      "Running Trial 5...\n",
      "0.8500000000000001 0.06557438524301998\n",
      "Running Trial 6...\n",
      "0.8599999999999999 0.08246211251235319\n",
      "Running Trial 7...\n",
      "0.85 0.0768114574786861\n",
      "Running Trial 8...\n",
      "0.8600000000000001 0.060000000000000005\n",
      "Running Trial 9...\n",
      "0.88 0.028284271247461926\n",
      "Running Trial 10...\n",
      "0.86 0.060000000000000005\n",
      "Running Trial 11...\n",
      "0.8300000000000001 0.05916079783099616\n",
      "Running Trial 12...\n",
      "0.87 0.051961524227066326\n",
      "Running Trial 13...\n",
      "0.8699999999999999 0.091104335791443\n",
      "Running Trial 14...\n",
      "0.84 0.12328828005937952\n",
      "Running Trial 15...\n",
      "0.83 0.07141428428542852\n",
      "Running Trial 16...\n",
      "0.84 0.028284271247461888\n",
      "Running Trial 17...\n",
      "0.8400000000000001 0.028284271247461888\n",
      "Running Trial 18...\n",
      "0.84 0.04898979485566356\n",
      "Running Trial 19...\n",
      "0.85 0.033166247903553984\n",
      "Running Trial 20...\n",
      "0.8399999999999999 0.08485281374238571\n",
      "Running Trial 21...\n",
      "0.84 0.028284271247461888\n",
      "Running Trial 22...\n",
      "0.8699999999999999 0.05916079783099614\n",
      "Running Trial 23...\n",
      "0.8500000000000001 0.07681145747868606\n",
      "Running Trial 24...\n",
      "0.84 0.04898979485566356\n",
      "Running Trial 25...\n",
      "0.8300000000000001 0.01732050807568874\n",
      "Running Trial 26...\n",
      "0.8999999999999999 0.06633249580710801\n",
      "Running Trial 27...\n",
      "0.85 0.099498743710662\n",
      "Running Trial 28...\n",
      "0.84 0.028284271247461888\n",
      "Running Trial 29...\n",
      "0.8600000000000001 0.044721359549995794\n",
      "Running Trial 30...\n",
      "0.8300000000000001 0.08660254037844385\n",
      "Running Trial 31...\n",
      "0.8099999999999999 0.12449899597988734\n",
      "Running Trial 32...\n",
      "0.8500000000000001 0.07141428428542851\n",
      "Running Trial 33...\n",
      "0.85 0.07681145747868606\n",
      "Running Trial 34...\n",
      "0.88 0.04898979485566356\n",
      "Running Trial 35...\n",
      "0.89 0.033166247903554026\n",
      "Running Trial 36...\n",
      "0.8300000000000001 0.01732050807568874\n",
      "Running Trial 37...\n",
      "0.8200000000000001 0.07211102550927981\n",
      "Running Trial 38...\n",
      "0.85 0.01732050807568879\n",
      "Running Trial 39...\n",
      "0.85 0.01732050807568879\n",
      "Running Trial 40...\n",
      "0.86 0.044721359549995794\n",
      "Running Trial 41...\n",
      "0.86 0.020000000000000018\n",
      "Running Trial 42...\n",
      "0.8600000000000001 0.060000000000000005\n",
      "Running Trial 43...\n",
      "0.8 0.06324555320336758\n",
      "Running Trial 44...\n",
      "0.88 0.028284271247461926\n",
      "Running Trial 45...\n",
      "0.87 0.01732050807568879\n",
      "Running Trial 46...\n",
      "0.8300000000000001 0.05916079783099616\n",
      "Running Trial 47...\n",
      "0.88 0.09797958971132713\n",
      "Running Trial 48...\n",
      "0.87 0.06557438524302002\n",
      "Running Trial 49...\n",
      "0.8700000000000001 0.06557438524302002\n",
      "Running Trial 50...\n",
      "0.8400000000000001 0.08000000000000002\n",
      "Running Trial 51...\n",
      "0.84 0.05656854249492382\n",
      "Running Trial 52...\n",
      "0.85 0.04358898943540674\n",
      "Running Trial 53...\n",
      "0.86 0.05999999999999998\n",
      "Running Trial 54...\n",
      "0.8500000000000001 0.09110433579144299\n",
      "Running Trial 55...\n",
      "0.8400000000000001 0.05656854249492382\n",
      "Running Trial 56...\n",
      "0.88 0.07483314773547882\n",
      "Running Trial 57...\n",
      "0.88 0.04898979485566356\n",
      "Running Trial 58...\n",
      "0.8200000000000001 0.06633249580710801\n",
      "Running Trial 59...\n",
      "0.8200000000000001 0.10770329614269009\n",
      "Running Trial 60...\n",
      "0.82 0.03464101615137753\n",
      "Running Trial 61...\n",
      "0.84 0.04898979485566356\n",
      "Running Trial 62...\n",
      "0.82 0.060000000000000005\n",
      "Running Trial 63...\n",
      "0.87 0.059160797830996134\n",
      "Running Trial 64...\n",
      "0.87 0.033166247903554026\n",
      "Running Trial 65...\n",
      "0.91 0.01732050807568879\n",
      "Running Trial 66...\n",
      "0.83 0.15066519173319365\n",
      "Running Trial 67...\n",
      "0.88 0.0\n",
      "Running Trial 68...\n",
      "0.87 0.033166247903554026\n",
      "Running Trial 69...\n",
      "0.89 0.033166247903554026\n",
      "Running Trial 70...\n",
      "0.8200000000000001 0.08246211251235322\n",
      "Running Trial 71...\n",
      "0.85 0.033166247903553984\n",
      "Running Trial 72...\n",
      "0.8700000000000001 0.04358898943540673\n",
      "Running Trial 73...\n",
      "0.88 0.07483314773547882\n",
      "Running Trial 74...\n",
      "0.84 0.07483314773547885\n",
      "Running Trial 75...\n",
      "0.8400000000000001 0.03999999999999998\n",
      "Running Trial 76...\n",
      "0.8899999999999999 0.043588989435406726\n",
      "Running Trial 77...\n",
      "0.8200000000000001 0.04472135954999579\n",
      "Running Trial 78...\n",
      "0.86 0.09165151389911681\n",
      "Running Trial 79...\n",
      "0.8500000000000001 0.05916079783099617\n",
      "Running Trial 80...\n",
      "0.87 0.01732050807568879\n",
      "Running Trial 81...\n",
      "0.8500000000000001 0.051961524227066305\n",
      "Running Trial 82...\n",
      "0.87 0.033166247903554026\n",
      "Running Trial 83...\n",
      "0.88 0.04898979485566356\n",
      "Running Trial 84...\n",
      "0.84 0.03999999999999998\n",
      "Running Trial 85...\n",
      "0.8799999999999999 0.07483314773547882\n",
      "Running Trial 86...\n",
      "0.87 0.033166247903554026\n",
      "Running Trial 87...\n",
      "0.87 0.01732050807568879\n",
      "Running Trial 88...\n",
      "0.8500000000000001 0.051961524227066305\n",
      "Running Trial 89...\n",
      "0.8600000000000001 0.06\n",
      "Running Trial 90...\n",
      "0.8500000000000001 0.07141428428542851\n",
      "Running Trial 91...\n",
      "0.89 0.01732050807568879\n",
      "Running Trial 92...\n",
      "0.89 0.01732050807568879\n",
      "Running Trial 93...\n",
      "0.8499999999999999 0.08660254037844387\n",
      "Running Trial 94...\n",
      "0.85 0.05916079783099617\n",
      "Running Trial 95...\n",
      "0.87 0.043588989435406726\n",
      "Running Trial 96...\n",
      "0.87 0.059160797830996134\n",
      "Running Trial 97...\n",
      "0.85 0.033166247903553984\n",
      "Running Trial 98...\n",
      "0.8400000000000001 0.0632455532033676\n",
      "Running Trial 99...\n",
      "0.88 0.028284271247461926\n",
      "Running Trial 100...\n",
      "0.88 0.040000000000000036\n"
     ]
    }
   ],
   "source": [
    "# Multi Layer Perceptron\n",
    "\n",
    "parameters = {'estimator__solver': ['lbfgs'], \n",
    "              'estimator__max_iter': [500, 1000, 1500, 2000, 2500, 3000], \n",
    "              'estimator__hidden_layer_sizes':np.linspace(10, 200, 10, dtype=int)\n",
    "             }\n",
    "\n",
    "mlp = MLPClassifier()\n",
    "\n",
    "mlp_nested = nested_cross_validation(mlp, parameters, X, Y, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "\n",
    "result_list = []\n",
    "result_columns = [\"Model\", \"Trial\", \"Accuracy\", \"Std_dev\"]\n",
    "\n",
    "\n",
    "for i in range(100):\n",
    "    result_list.append([\"NB\", i, nb_nested[i][0], nb_nested[i][1]])\n",
    "    \n",
    "nb_df = pd.DataFrame(result_list,columns=result_columns)\n",
    "\n",
    "nb_df.to_csv(\"NB_results\")\n",
    "\n",
    "result_list = []\n",
    "result_columns = [\"Model\", \"Trial\", \"Accuracy\", \"Std_dev\"]\n",
    "\n",
    "for i in range(100):\n",
    "    result_list.append([\"RF\", i, rf_nested[i][0], rf_nested[i][1]])\n",
    "    \n",
    "rf_df = pd.DataFrame(result_list,columns=result_columns)\n",
    "\n",
    "rf_df.to_csv(\"RF_results\")\n",
    "\n",
    "result_list = []\n",
    "result_columns = [\"Model\", \"Trial\", \"Accuracy\", \"Std_dev\"]\n",
    "\n",
    "for i in range(100):\n",
    "    result_list.append([\"MLP\", i, mlp_nested[i][0], mlp_nested[i][1]])\n",
    "    \n",
    "mlp_df = pd.DataFrame(result_list,columns=result_columns)\n",
    "\n",
    "mlp_df.to_csv(\"MLP_results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB average:  0.8143\n",
      "RF average:  0.7614\n",
      "MLP average:  0.8541\n"
     ]
    }
   ],
   "source": [
    "# 3. Visualise boxplot\n",
    "\n",
    "def read_results(csvfile):\n",
    "    results = []\n",
    "\n",
    "    with open(csvfile, \"r\") as csv_file:\n",
    "        next(csv_file)\n",
    "        for line in csv_file:\n",
    "            temp = line.strip()\n",
    "            temp = temp.split(\",\")\n",
    "            model = temp[1]\n",
    "            results.append(float(temp[-2]))\n",
    "        \n",
    "    return model, results\n",
    "\n",
    "mod1, res1 = read_results(\"NB_results\")\n",
    "mod2, res2 = read_results(\"RF_results\")\n",
    "mod3, res3 = read_results(\"MLP_results\")\n",
    "# res4 = all_scores\n",
    "\n",
    "plt.boxplot([res1, res2, res3], vert = 0, labels=[\"NB\", \"RF\", \"MLP\"])\n",
    "plt.title(\"Model Comparison\")\n",
    "plt.xlabel(\"Accuracy\")\n",
    "plt.savefig(\"boxplot.png\", dpi=300)\n",
    "plt.show()\n",
    "\n",
    "print(\"NB average: \", np.average(np.array(nb_nested)[:,0]))\n",
    "print(\"RF average: \", np.average(np.array(rf_nested)[:,0]))\n",
    "print(\"MLP average: \", np.average(np.array(mlp_nested)[:,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature ranking:\n",
      "1. feature 2184 (0.045021)\n",
      "2. feature 2223 (0.007394)\n",
      "3. feature 854 (0.007225)\n",
      "4. feature 2213 (0.005273)\n",
      "5. feature 2205 (0.005061)\n",
      "6. feature 744 (0.004983)\n",
      "7. feature 849 (0.004852)\n",
      "8. feature 2104 (0.004329)\n",
      "9. feature 1065 (0.004279)\n",
      "10. feature 842 (0.004258)\n",
      "11. feature 2224 (0.004163)\n",
      "12. feature 2735 (0.004163)\n",
      "13. feature 2751 (0.003794)\n",
      "14. feature 1657 (0.003745)\n",
      "15. feature 2214 (0.003635)\n",
      "16. feature 486 (0.003634)\n",
      "17. feature 839 (0.003607)\n",
      "18. feature 1909 (0.003435)\n",
      "19. feature 1973 (0.003401)\n",
      "20. feature 2209 (0.003386)\n",
      "21. feature 1027 (0.003360)\n",
      "22. feature 864 (0.003322)\n",
      "23. feature 2020 (0.003273)\n",
      "24. feature 2207 (0.003145)\n",
      "25. feature 746 (0.003112)\n",
      "26. feature 1000 (0.003096)\n",
      "27. feature 1670 (0.002996)\n",
      "28. feature 2210 (0.002980)\n",
      "29. feature 386 (0.002874)\n",
      "30. feature 1957 (0.002732)\n",
      "31. feature 2113 (0.002723)\n",
      "32. feature 485 (0.002682)\n",
      "33. feature 2732 (0.002675)\n",
      "34. feature 2753 (0.002663)\n",
      "35. feature 1068 (0.002632)\n",
      "36. feature 2105 (0.002597)\n",
      "37. feature 2729 (0.002561)\n",
      "38. feature 1665 (0.002533)\n",
      "39. feature 814 (0.002531)\n",
      "40. feature 1655 (0.002498)\n",
      "41. feature 484 (0.002484)\n",
      "42. feature 2241 (0.002436)\n",
      "43. feature 2220 (0.002427)\n",
      "44. feature 1646 (0.002426)\n",
      "45. feature 2754 (0.002405)\n",
      "46. feature 724 (0.002396)\n",
      "47. feature 1057 (0.002354)\n",
      "48. feature 1071 (0.002349)\n",
      "49. feature 747 (0.002339)\n",
      "50. feature 58 (0.002338)\n",
      "51. feature 736 (0.002301)\n",
      "52. feature 2195 (0.002266)\n",
      "53. feature 844 (0.002260)\n",
      "54. feature 850 (0.002195)\n",
      "55. feature 863 (0.002166)\n",
      "56. feature 464 (0.002157)\n",
      "57. feature 2263 (0.002140)\n",
      "58. feature 1067 (0.002114)\n",
      "59. feature 261 (0.002110)\n",
      "60. feature 836 (0.002100)\n",
      "61. feature 1662 (0.002094)\n",
      "62. feature 1649 (0.002093)\n",
      "63. feature 120 (0.002080)\n",
      "64. feature 731 (0.002041)\n",
      "65. feature 1967 (0.002031)\n",
      "66. feature 860 (0.002031)\n",
      "67. feature 2596 (0.002029)\n",
      "68. feature 2740 (0.002019)\n",
      "69. feature 2221 (0.002005)\n",
      "70. feature 1060 (0.002005)\n",
      "71. feature 672 (0.001959)\n",
      "72. feature 2170 (0.001957)\n",
      "73. feature 2212 (0.001952)\n",
      "74. feature 459 (0.001950)\n",
      "75. feature 1685 (0.001929)\n",
      "76. feature 846 (0.001922)\n",
      "77. feature 708 (0.001920)\n",
      "78. feature 1567 (0.001918)\n",
      "79. feature 1061 (0.001913)\n",
      "80. feature 2276 (0.001909)\n",
      "81. feature 853 (0.001899)\n",
      "82. feature 1064 (0.001895)\n",
      "83. feature 2767 (0.001892)\n",
      "84. feature 2022 (0.001892)\n",
      "85. feature 701 (0.001875)\n",
      "86. feature 2551 (0.001872)\n",
      "87. feature 2749 (0.001861)\n",
      "88. feature 2285 (0.001835)\n",
      "89. feature 1959 (0.001832)\n",
      "90. feature 1677 (0.001818)\n",
      "91. feature 1034 (0.001817)\n",
      "92. feature 2124 (0.001811)\n",
      "93. feature 2019 (0.001807)\n",
      "94. feature 1200 (0.001802)\n",
      "95. feature 2404 (0.001777)\n",
      "96. feature 2219 (0.001775)\n",
      "97. feature 1690 (0.001772)\n",
      "98. feature 805 (0.001730)\n",
      "99. feature 2739 (0.001712)\n",
      "100. feature 301 (0.001703)\n",
      "101. feature 1527 (0.001700)\n",
      "102. feature 228 (0.001698)\n",
      "103. feature 2204 (0.001695)\n",
      "104. feature 1005 (0.001683)\n",
      "105. feature 2326 (0.001681)\n",
      "106. feature 1899 (0.001675)\n",
      "107. feature 2025 (0.001665)\n",
      "108. feature 2109 (0.001663)\n",
      "109. feature 748 (0.001659)\n",
      "110. feature 2027 (0.001656)\n",
      "111. feature 2063 (0.001654)\n",
      "112. feature 1905 (0.001649)\n",
      "113. feature 833 (0.001648)\n",
      "114. feature 2733 (0.001645)\n",
      "115. feature 1106 (0.001640)\n",
      "116. feature 1862 (0.001639)\n",
      "117. feature 1679 (0.001639)\n",
      "118. feature 2260 (0.001629)\n",
      "119. feature 734 (0.001627)\n",
      "120. feature 1719 (0.001624)\n",
      "121. feature 1669 (0.001614)\n",
      "122. feature 1645 (0.001612)\n",
      "123. feature 2225 (0.001604)\n",
      "124. feature 2038 (0.001590)\n",
      "125. feature 307 (0.001586)\n",
      "126. feature 999 (0.001584)\n",
      "127. feature 2640 (0.001576)\n",
      "128. feature 1676 (0.001576)\n",
      "129. feature 1344 (0.001568)\n",
      "130. feature 1773 (0.001564)\n",
      "131. feature 146 (0.001556)\n",
      "132. feature 1003 (0.001554)\n",
      "133. feature 2230 (0.001553)\n",
      "134. feature 2720 (0.001532)\n",
      "135. feature 1663 (0.001532)\n",
      "136. feature 179 (0.001526)\n",
      "137. feature 870 (0.001516)\n",
      "138. feature 841 (0.001507)\n",
      "139. feature 1408 (0.001507)\n",
      "140. feature 2825 (0.001506)\n",
      "141. feature 2750 (0.001501)\n",
      "142. feature 1306 (0.001500)\n",
      "143. feature 837 (0.001496)\n",
      "144. feature 2087 (0.001495)\n",
      "145. feature 799 (0.001495)\n",
      "146. feature 2742 (0.001494)\n",
      "147. feature 2723 (0.001492)\n",
      "148. feature 1572 (0.001487)\n",
      "149. feature 2678 (0.001486)\n",
      "150. feature 205 (0.001479)\n",
      "151. feature 2669 (0.001477)\n",
      "152. feature 835 (0.001459)\n",
      "153. feature 75 (0.001453)\n",
      "154. feature 2021 (0.001451)\n",
      "155. feature 732 (0.001449)\n",
      "156. feature 2175 (0.001449)\n",
      "157. feature 481 (0.001441)\n",
      "158. feature 1160 (0.001424)\n",
      "159. feature 1883 (0.001423)\n",
      "160. feature 1274 (0.001419)\n",
      "161. feature 2359 (0.001415)\n",
      "162. feature 1772 (0.001414)\n",
      "163. feature 1911 (0.001413)\n",
      "164. feature 1144 (0.001411)\n",
      "165. feature 2200 (0.001410)\n",
      "166. feature 1838 (0.001405)\n",
      "167. feature 2190 (0.001394)\n",
      "168. feature 2049 (0.001393)\n",
      "169. feature 866 (0.001393)\n",
      "170. feature 2656 (0.001391)\n",
      "171. feature 905 (0.001389)\n",
      "172. feature 462 (0.001386)\n",
      "173. feature 1952 (0.001386)\n",
      "174. feature 1914 (0.001381)\n",
      "175. feature 447 (0.001364)\n",
      "176. feature 2807 (0.001363)\n",
      "177. feature 2264 (0.001362)\n",
      "178. feature 1056 (0.001360)\n",
      "179. feature 840 (0.001359)\n",
      "180. feature 2026 (0.001354)\n",
      "181. feature 1030 (0.001352)\n",
      "182. feature 2077 (0.001351)\n",
      "183. feature 2208 (0.001345)\n",
      "184. feature 2752 (0.001332)\n",
      "185. feature 1651 (0.001330)\n",
      "186. feature 2107 (0.001328)\n",
      "187. feature 1972 (0.001326)\n",
      "188. feature 1270 (0.001325)\n",
      "189. feature 1088 (0.001322)\n",
      "190. feature 461 (0.001322)\n",
      "191. feature 2705 (0.001321)\n",
      "192. feature 1053 (0.001321)\n",
      "193. feature 460 (0.001319)\n",
      "194. feature 1641 (0.001318)\n",
      "195. feature 1691 (0.001306)\n",
      "196. feature 1753 (0.001300)\n",
      "197. feature 623 (0.001285)\n",
      "198. feature 1656 (0.001280)\n",
      "199. feature 1610 (0.001275)\n",
      "200. feature 2567 (0.001272)\n",
      "201. feature 67 (0.001269)\n",
      "202. feature 742 (0.001267)\n",
      "203. feature 2227 (0.001265)\n",
      "204. feature 1424 (0.001254)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "shape mismatch: objects cannot be broadcast to a single shape",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-145-85de23eb5e97>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Feature importances\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m plt.bar(range(X.shape[1]), importances[indices],\n\u001b[1;32m---> 29\u001b[1;33m        color=\"r\", yerr=std[indices], align=\"center\")\n\u001b[0m\u001b[0;32m     30\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxticks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxlim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\matplotlib\\pyplot.py\u001b[0m in \u001b[0;36mbar\u001b[1;34m(x, height, width, bottom, align, data, **kwargs)\u001b[0m\n\u001b[0;32m   2407\u001b[0m     return gca().bar(\n\u001b[0;32m   2408\u001b[0m         \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mwidth\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbottom\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbottom\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malign\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0malign\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2409\u001b[1;33m         **({\"data\": data} if data is not None else {}), **kwargs)\n\u001b[0m\u001b[0;32m   2410\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2411\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\matplotlib\\__init__.py\u001b[0m in \u001b[0;36minner\u001b[1;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1563\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0minner\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0max\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1564\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1565\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0max\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msanitize_sequence\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1566\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1567\u001b[0m         \u001b[0mbound\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew_sig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0max\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\matplotlib\\axes\\_axes.py\u001b[0m in \u001b[0;36mbar\u001b[1;34m(self, x, height, width, bottom, align, **kwargs)\u001b[0m\n\u001b[0;32m   2339\u001b[0m         x, height, width, y, linewidth = np.broadcast_arrays(\n\u001b[0;32m   2340\u001b[0m             \u001b[1;31m# Make args iterable too.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2341\u001b[1;33m             np.atleast_1d(x), height, width, y, linewidth)\n\u001b[0m\u001b[0;32m   2342\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2343\u001b[0m         \u001b[1;31m# Now that units have been converted, set the tick locations.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mbroadcast_arrays\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\numpy\\lib\\stride_tricks.py\u001b[0m in \u001b[0;36mbroadcast_arrays\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    262\u001b[0m     \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_m\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msubok\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msubok\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0m_m\u001b[0m \u001b[1;32min\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    263\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 264\u001b[1;33m     \u001b[0mshape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_broadcast_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    265\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    266\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mshape\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0marray\u001b[0m \u001b[1;32min\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\numpy\\lib\\stride_tricks.py\u001b[0m in \u001b[0;36m_broadcast_shape\u001b[1;34m(*args)\u001b[0m\n\u001b[0;32m    189\u001b[0m     \u001b[1;31m# use the old-iterator because np.nditer does not handle size 0 arrays\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    190\u001b[0m     \u001b[1;31m# consistently\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 191\u001b[1;33m     \u001b[0mb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbroadcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    192\u001b[0m     \u001b[1;31m# unfortunately, it cannot handle 32 or more arguments directly\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    193\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mpos\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m31\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: shape mismatch: objects cannot be broadcast to a single shape"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Find most significant feature by using Random Forest feature importance\n",
    "\n",
    "X_total = df_train.drop(['Chromosome', 'Start','End', 'Nclone'], axis=1)\n",
    "X_total = X_total.transpose()\n",
    "\n",
    "originalclass = []\n",
    "predictedclass = []\n",
    "\n",
    "forest = RandomForestClassifier(n_estimators=100)\n",
    "# temp = evaluate_model(random_forest, X, Y)\n",
    "\n",
    "forest.fit(X_total, Y)\n",
    "# print(random_forest.feature_importances_.max())\n",
    "importances = forest.feature_importances_\n",
    "std = np.std([tree.feature_importances_ for tree in forest.estimators_],\n",
    "             axis=0)\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "# Print the feature ranking\n",
    "print(\"Feature ranking:\")\n",
    "\n",
    "for f in range(X.shape[1]):\n",
    "    print(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]]))\n",
    "\n",
    "# Plot the feature importances of the forest\n",
    "plt.figure()\n",
    "plt.title(\"Feature importances\")\n",
    "plt.bar(range(X.shape[1]), importances[indices],\n",
    "       color=\"r\", yerr=std[indices], align=\"center\")\n",
    "plt.xticks(range(X.shape[1]), indices)\n",
    "plt.xlim([-1, X.shape[1]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load validation set and take features\n",
    "\n",
    "temp_list = []\n",
    "\n",
    "with open(\"Validation_call.txt\", 'r') as temp:\n",
    "    for line in temp:\n",
    "        temp_list.append(line.split())\n",
    "\n",
    "columns_temp = temp_list[0]\n",
    "columns_temp = [x.replace(\"\\\"\", \"\") for x in columns_temp]\n",
    "\n",
    "df_test = pd.DataFrame(temp_list[1:], columns=columns_temp)\n",
    "test_set = df_test.drop(['Chromosome', 'Start','End', 'Nclone'], axis=1)\n",
    "test_set = test_set.transpose()\n",
    "\n",
    "for i, val in enumerate(p_val):\n",
    "    if val >= threshold and i not in important_genes:\n",
    "        test_set = test_set.drop([i], axis=1)\n",
    "        \n",
    "patient_ids = test_set.index.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 1\n",
      "Trial 2\n",
      "Trial 3\n",
      "Trial 4\n",
      "Trial 5\n",
      "Trial 6\n",
      "Trial 7\n",
      "Trial 8\n",
      "Trial 9\n",
      "Trial 10\n",
      "Trial 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\folke\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (250) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\folke\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (250) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\folke\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (250) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 12\n",
      "Trial 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\folke\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\folke\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 14\n",
      "Trial 15\n",
      "Trial 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\folke\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\folke\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 17\n",
      "Trial 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\folke\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\folke\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\folke\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\folke\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 20\n",
      "Trial 21\n",
      "Trial 22\n",
      "Trial 23\n",
      "Trial 24\n",
      "Trial 25\n",
      "Trial 26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\folke\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\folke\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\folke\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\folke\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\folke\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\folke\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 28\n",
      "Trial 29\n",
      "Trial 30\n",
      "Trial 31\n",
      "Trial 32\n",
      "Trial 33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\folke\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\folke\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 34\n",
      "Trial 35\n",
      "Trial 36\n",
      "Trial 37\n",
      "Trial 38\n",
      "Trial 39\n",
      "Trial 40\n",
      "Trial 41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\folke\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\folke\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\folke\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\folke\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (2000) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\folke\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (2000) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 43\n",
      "Trial 44\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\folke\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (250) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\folke\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (250) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\folke\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (250) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\folke\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (250) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\folke\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (250) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\folke\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (250) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 46\n",
      "Trial 47\n",
      "Trial 48\n",
      "Trial 49\n",
      "Trial 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\folke\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\folke\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 51\n",
      "Trial 52\n",
      "Trial 53\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\folke\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\folke\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\folke\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 54\n",
      "Trial 55\n",
      "Trial 56\n",
      "Trial 57\n",
      "Trial 58\n",
      "Trial 59\n",
      "Trial 60\n",
      "Trial 61\n",
      "Trial 62\n",
      "Trial 63\n",
      "Trial 64\n",
      "Trial 65\n",
      "Trial 66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\folke\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1500) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\folke\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1500) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\folke\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1500) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 67\n",
      "Trial 68\n",
      "Trial 69\n",
      "Trial 70\n",
      "Trial 71\n",
      "Trial 72\n",
      "Trial 73\n",
      "Trial 74\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\folke\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\folke\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\folke\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 75\n",
      "Trial 76\n",
      "Trial 77\n",
      "Trial 78\n",
      "Trial 79\n",
      "Trial 80\n",
      "Trial 81\n",
      "Trial 82\n",
      "Trial 83\n",
      "Trial 84\n",
      "Trial 85\n",
      "Trial 86\n",
      "Trial 87\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\folke\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\folke\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\folke\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 88\n",
      "Trial 89\n",
      "Trial 90\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\folke\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\folke\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 91\n",
      "Trial 92\n",
      "Trial 93\n",
      "Trial 94\n",
      "Trial 95\n",
      "Trial 96\n",
      "Trial 97\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\folke\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\folke\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\folke\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 98\n",
      "Trial 99\n",
      "Trial 100\n"
     ]
    }
   ],
   "source": [
    "# Train and predict with MLP-model 100 times\n",
    "\n",
    "prediction_total = []\n",
    "\n",
    "for i in range(100):\n",
    "    print(\"Trial {}\".format(i + 1))\n",
    "    mlp = MLPClassifier()\n",
    "\n",
    "    folds = StratifiedKFold(n_splits=10, shuffle=True, random_state=i)\n",
    "\n",
    "    onevsrest_mlp = OneVsRestClassifier(mlp)\n",
    "\n",
    "    model = RandomizedSearchCV(estimator=onevsrest_mlp, param_distributions=parameters_mlp, cv=folds, n_jobs=2)\n",
    "    model.fit(X, Y)\n",
    "    predictions = model.predict(test_set)\n",
    "    prediction_total.append(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_temp = []\n",
    "predictions = []\n",
    "\n",
    "for i in range(57):\n",
    "    certainty = \"Uncertain\"\n",
    "    \n",
    "    temp = (np.array(prediction_total)[:,i])\n",
    "    temp1 = 0\n",
    "    temp2 = 0\n",
    "    temp3 = 0\n",
    "    for j in temp:\n",
    "        if j == 1:\n",
    "            temp1 += 1\n",
    "        if j == 2:\n",
    "            temp2 += 1\n",
    "        if j == 3:\n",
    "            temp3 += 1\n",
    "    \n",
    "    if temp1 >= 80 or temp2 >= 80 or temp3 >= 80:\n",
    "        certainty = \"Certain\"\n",
    "    \n",
    "    output_temp.append([patient_ids[i], temp1 / 100, temp2 / 100, temp3 / 100, certainty])\n",
    "    predictions.append(np.argmax([temp1, temp2, temp3]) + 1)\n",
    "\n",
    "total_mlp = pd.DataFrame(output_temp, columns=[\"Patient\", \"HR+\", \"HER2+\", \"Triple Neg\", \"Certain?\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76\n"
     ]
    }
   ],
   "source": [
    "for i, p in enumerate(prediction_total):\n",
    "    if np.count_nonzero(predictions - p) == 0:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters_mlp = {'estimator__activation':['identity', 'logistic', 'tanh', 'relu'],\n",
    "              'estimator__solver': ['lbfgs', 'sgd', 'adam'], \n",
    "              'estimator__max_iter': [100, 250, 500, 1000, 1500, 2000, 2500, 3000], \n",
    "              'estimator__hidden_layer_sizes':np.linspace(10, 200, 20, dtype=int)\n",
    "             }\n",
    "\n",
    "mlp = MLPClassifier()\n",
    "\n",
    "folds = StratifiedKFold(n_splits=10, shuffle=True, random_state=76)\n",
    "\n",
    "onevsrest_mlp = OneVsRestClassifier(mlp)\n",
    "\n",
    "model = RandomizedSearchCV(estimator=onevsrest_mlp, param_distributions=parameters_mlp, cv=folds, n_jobs=2)\n",
    "model.fit(X, Y)\n",
    "predictions = model.predict(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Array.1 \t HR+\n",
      "Array.61 \t HR+\n",
      "Array.70 \t Triple Neg\n",
      "Array.14 \t HER2+\n",
      "Array.91 \t Triple Neg\n",
      "Array.58 \t HER2+\n",
      "Array.140 \t HER2+\n",
      "Array.20 \t HR+\n",
      "Array.133 \t Triple Neg\n",
      "Array.77 \t HER2+\n",
      "Array.131 \t HER2+\n",
      "Array.84 \t HR+\n",
      "Array.44 \t Triple Neg\n",
      "Array.41 \t HER2+\n",
      "Array.29 \t HR+\n",
      "Array.150 \t HER2+\n",
      "Array.151 \t HR+\n",
      "Array.132 \t HR+\n",
      "Array.32 \t HR+\n",
      "Array.11 \t HR+\n",
      "Array.156 \t Triple Neg\n",
      "Array.80 \t HER2+\n",
      "Array.9 \t Triple Neg\n",
      "Array.3 \t HR+\n",
      "Array.136 \t HR+\n",
      "Array.46 \t HER2+\n",
      "Array.109 \t HER2+\n",
      "Array.103 \t Triple Neg\n",
      "Array.97 \t HR+\n",
      "Array.40 \t HR+\n",
      "Array.147 \t HER2+\n",
      "Array.161 \t Triple Neg\n",
      "Array.127 \t Triple Neg\n",
      "Array.119 \t Triple Neg\n",
      "Array.157 \t HER2+\n",
      "Array.54 \t HER2+\n",
      "Array.115 \t HR+\n",
      "Array.121 \t HER2+\n",
      "Array.122 \t HER2+\n",
      "Array.158 \t HER2+\n",
      "Array.87 \t HER2+\n",
      "Array.28 \t HER2+\n",
      "Array.45 \t HR+\n",
      "Array.155 \t HR+\n",
      "Array.12 \t Triple Neg\n",
      "Array.74 \t HER2+\n",
      "Array.26 \t HR+\n",
      "Array.120 \t HER2+\n",
      "Array.108 \t HR+\n",
      "Array.81 \t HER2+\n",
      "Array.126 \t HR+\n",
      "Array.92 \t HR+\n",
      "Array.160 \t Triple Neg\n",
      "Array.66 \t Triple Neg\n",
      "Array.83 \t HR+\n",
      "Array.128 \t HER2+\n",
      "Array.63 \t Triple Neg\n"
     ]
    }
   ],
   "source": [
    "output_preds = []\n",
    "\n",
    "for i, pred in enumerate(predictions):\n",
    "    if pred == 1:\n",
    "        output_preds.append([patient_ids[i],\"HER2+\"])\n",
    "    if pred == 2:\n",
    "        output_preds.append([patient_ids[i],\"HR+\"])\n",
    "    if pred == 3:\n",
    "        output_preds.append([patient_ids[i],\"Triple Neg\"])\n",
    "\n",
    "file1 = open(\"predictions.txt\",\"w\") \n",
    "        \n",
    "file1.write(\"\\\"Sample\\\" \\t \\\"Subgroup\\\"\\n\")\n",
    "for i, prediction in enumerate(predictions):\n",
    "    print(output_preds[i][0], \"\\t\", output_preds[i][1])\n",
    "    file1.write(\"\\\"{0}\\\" \\t \\\"{1}\\\"\\n\".format(output_preds[i][0], output_preds[i][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dump model\n",
    "\n",
    "pkl_filename = \"cats_12_model.pkl\"\n",
    "with open(pkl_filename, 'wb') as file:\n",
    "    pickle.dump(model, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
